import { test, suite } from "node:test";
import assert from "node:assert";
import { tokenise } from "./tokeniser.mjs";

suite("Indexer / Tokeniser", () => {
  test("Check empty", () => {
    check("", []);
  });

  test("Single word", () => {
    check("bob", [
      {
        index: 0,
        segment: "bob",
      },
    ]);
  });

  test("Multiple words", () => {
    check("bob smith", [
      {
        index: 0,
        segment: "bob",
      },
      {
        index: 4,
        segment: "smith",
      },
    ]);
  });

  test("More complex example", () => {
    check("The quick (â€œbrownâ€) fox can't jump 32.3 feet, right?", [
      { index: 0, segment: "The" },
      { index: 4, segment: "quick" },
      { index: 12, segment: "brown" },
      { index: 20, segment: "fox" },
      { index: 24, segment: "can't" },
      { index: 30, segment: "jump" },
      { index: 35, segment: "32.3" },
      { index: 40, segment: "feet" },
      { index: 46, segment: "right" },
    ]);
  });

  test("Japanese", () => {
    check("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ãŸã¬ãã€‚", [
      { index: 0, segment: "å¾è¼©" },
      { index: 2, segment: "ã¯" },
      { index: 3, segment: "çŒ«" },
      { index: 4, segment: "ã§" },
      { index: 5, segment: "ã‚ã‚‹" },
      { index: 8, segment: "åå‰" },
      { index: 10, segment: "ã¯" },
      { index: 11, segment: "ãŸã¬ã" },
    ]);
  });

  test("hebrew", () => {
    check("××™×š ×‘×œ×© ×ª×¤×¡ ×’×ž×“ ×¨×•×¦×— ×¢×– ×§×˜× ×”?", [
      { index: 0, segment: "××™×š" },
      { index: 4, segment: "×‘×œ×©" },
      { index: 8, segment: "×ª×¤×¡" },
      { index: 12, segment: "×’×ž×“" },
      { index: 16, segment: "×¨×•×¦×—" },
      { index: 21, segment: "×¢×–" },
      { index: 24, segment: "×§×˜× ×”" },
    ]);
  });

  test("nÃªhiyawÃªwin", () => {
    check("á‘•á» á’¥á”ª á‘­á“¯á‘²á¤ áŠá“„á¦á¨á™®", [
      { index: 0, segment: "á‘•á»" },
      { index: 3, segment: "á’¥á”ª" },
      { index: 6, segment: "á‘­á“¯á‘²á¤" },
      { index: 11, segment: "áŠá“„á¦á¨" },
    ]);
  });

  test("emojis", () => {
    check(
      "ðŸ«£ðŸ«µðŸ‘¨â€ðŸ‘¨â€ðŸ‘¦â€ðŸ‘¦",
      [
        {
          index: 0,
          segment: "ðŸ«£",
        },
        {
          index: 2,
          segment: "ðŸ«µ",
        },
        {
          index: 4,
          segment: "ðŸ‘¨â€ðŸ‘¨â€ðŸ‘¦â€ðŸ‘¦",
        },
      ],
      true
    );
  });

  const check = (word, expected, includeNonWords) => {
    const result = tokenise(word, includeNonWords);
    assert.deepEqual(Array.from(result), expected);
  };
});
